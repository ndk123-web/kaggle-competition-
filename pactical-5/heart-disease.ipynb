{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":125192,"databundleVersionId":15408205,"sourceType":"competition"}],"dockerImageVersionId":31259,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-12T06:29:34.911286Z","iopub.execute_input":"2026-02-12T06:29:34.912100Z","iopub.status.idle":"2026-02-12T06:29:34.924049Z","shell.execute_reply.started":"2026-02-12T06:29:34.912056Z","shell.execute_reply":"2026-02-12T06:29:34.922835Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s6e2/sample_submission.csv\n/kaggle/input/playground-series-s6e2/train.csv\n/kaggle/input/playground-series-s6e2/test.csv\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import pandas as pd\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T06:45:05.230852Z","iopub.execute_input":"2026-02-12T06:45:05.231149Z","iopub.status.idle":"2026-02-12T06:45:05.237678Z","shell.execute_reply.started":"2026-02-12T06:45:05.231125Z","shell.execute_reply":"2026-02-12T06:45:05.236870Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/playground-series-s6e2/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/playground-series-s6e2/test.csv\")\ntest.head()\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T06:29:34.951602Z","iopub.execute_input":"2026-02-12T06:29:34.952130Z","iopub.status.idle":"2026-02-12T06:29:35.586273Z","shell.execute_reply.started":"2026-02-12T06:29:34.952076Z","shell.execute_reply":"2026-02-12T06:29:35.585319Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"   id  Age  Sex  Chest pain type   BP  Cholesterol  FBS over 120  EKG results  \\\n0   0   58    1                4  152          239             0            0   \n1   1   52    1                1  125          325             0            2   \n2   2   56    0                2  160          188             0            2   \n3   3   44    0                3  134          229             0            2   \n4   4   58    1                4  140          234             0            2   \n\n   Max HR  Exercise angina  ST depression  Slope of ST  \\\n0     158                1            3.6            2   \n1     171                0            0.0            1   \n2     151                0            0.0            1   \n3     150                0            1.0            2   \n4     125                1            3.8            2   \n\n   Number of vessels fluro  Thallium Heart Disease  \n0                        2         7      Presence  \n1                        0         3       Absence  \n2                        0         3       Absence  \n3                        0         3       Absence  \n4                        3         3      Presence  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>Chest pain type</th>\n      <th>BP</th>\n      <th>Cholesterol</th>\n      <th>FBS over 120</th>\n      <th>EKG results</th>\n      <th>Max HR</th>\n      <th>Exercise angina</th>\n      <th>ST depression</th>\n      <th>Slope of ST</th>\n      <th>Number of vessels fluro</th>\n      <th>Thallium</th>\n      <th>Heart Disease</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>58</td>\n      <td>1</td>\n      <td>4</td>\n      <td>152</td>\n      <td>239</td>\n      <td>0</td>\n      <td>0</td>\n      <td>158</td>\n      <td>1</td>\n      <td>3.6</td>\n      <td>2</td>\n      <td>2</td>\n      <td>7</td>\n      <td>Presence</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>52</td>\n      <td>1</td>\n      <td>1</td>\n      <td>125</td>\n      <td>325</td>\n      <td>0</td>\n      <td>2</td>\n      <td>171</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Absence</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>56</td>\n      <td>0</td>\n      <td>2</td>\n      <td>160</td>\n      <td>188</td>\n      <td>0</td>\n      <td>2</td>\n      <td>151</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Absence</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>44</td>\n      <td>0</td>\n      <td>3</td>\n      <td>134</td>\n      <td>229</td>\n      <td>0</td>\n      <td>2</td>\n      <td>150</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Absence</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>58</td>\n      <td>1</td>\n      <td>4</td>\n      <td>140</td>\n      <td>234</td>\n      <td>0</td>\n      <td>2</td>\n      <td>125</td>\n      <td>1</td>\n      <td>3.8</td>\n      <td>2</td>\n      <td>3</td>\n      <td>3</td>\n      <td>Presence</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"X = train.drop(columns=[\"Heart Disease\"])\ny = train[\"Heart Disease\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T06:49:28.708375Z","iopub.execute_input":"2026-02-12T06:49:28.708638Z","iopub.status.idle":"2026-02-12T06:49:28.737960Z","shell.execute_reply.started":"2026-02-12T06:49:28.708620Z","shell.execute_reply":"2026-02-12T06:49:28.736752Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/1836974315.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Heart Disease\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Heart Disease\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6301\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'columns'"],"ename":"AttributeError","evalue":"'Series' object has no attribute 'columns'","output_type":"error"}],"execution_count":55},{"cell_type":"code","source":"y.value_counts(normalize=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T06:29:35.611888Z","iopub.execute_input":"2026-02-12T06:29:35.612221Z","iopub.status.idle":"2026-02-12T06:29:35.652687Z","shell.execute_reply.started":"2026-02-12T06:29:35.612194Z","shell.execute_reply":"2026-02-12T06:29:35.651880Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"Heart Disease\nAbsence     0.55166\nPresence    0.44834\nName: proportion, dtype: float64"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T06:29:35.653713Z","iopub.execute_input":"2026-02-12T06:29:35.653982Z","iopub.status.idle":"2026-02-12T06:29:35.703961Z","shell.execute_reply.started":"2026-02-12T06:29:35.653953Z","shell.execute_reply":"2026-02-12T06:29:35.702658Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"id                         0\nAge                        0\nSex                        0\nChest pain type            0\nBP                         0\nCholesterol                0\nFBS over 120               0\nEKG results                0\nMax HR                     0\nExercise angina            0\nST depression              0\nSlope of ST                0\nNumber of vessels fluro    0\nThallium                   0\nHeart Disease              0\ndtype: int64"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"train.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T06:29:35.705290Z","iopub.execute_input":"2026-02-12T06:29:35.705556Z","iopub.status.idle":"2026-02-12T06:29:35.712258Z","shell.execute_reply.started":"2026-02-12T06:29:35.705530Z","shell.execute_reply":"2026-02-12T06:29:35.711413Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"id                           int64\nAge                          int64\nSex                          int64\nChest pain type              int64\nBP                           int64\nCholesterol                  int64\nFBS over 120                 int64\nEKG results                  int64\nMax HR                       int64\nExercise angina              int64\nST depression              float64\nSlope of ST                  int64\nNumber of vessels fluro      int64\nThallium                     int64\nHeart Disease               object\ndtype: object"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T06:32:42.351184Z","iopub.execute_input":"2026-02-12T06:32:42.351904Z","iopub.status.idle":"2026-02-12T06:32:42.356848Z","shell.execute_reply.started":"2026-02-12T06:32:42.351865Z","shell.execute_reply":"2026-02-12T06:32:42.356107Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# Since There is no NaN values\n# for col in X.columns:\n#     if X[col].dtype == \"object\":\n#         X[col].fillna(X[col].mode()[0], inplace=True)\n#         test[col].fillna(test[col].mode()[0], inplace=True)\n#     else:\n#         X[col].fillna(X[col].median(), inplace=True)\n#         test[col].fillna(test[col].median(), inplace=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T06:32:23.625730Z","iopub.execute_input":"2026-02-12T06:32:23.626274Z","iopub.status.idle":"2026-02-12T06:32:23.629676Z","shell.execute_reply.started":"2026-02-12T06:32:23.626251Z","shell.execute_reply":"2026-02-12T06:32:23.628857Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"for col in X.select_dtypes(include=\"object\").columns:\n    le = LabelEncoder()\n    X[col] = le.fit_transform(X[col])\n    test[col] = le.transform(test[col])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T06:32:25.286504Z","iopub.execute_input":"2026-02-12T06:32:25.286760Z","iopub.status.idle":"2026-02-12T06:32:25.291863Z","shell.execute_reply.started":"2026-02-12T06:32:25.286739Z","shell.execute_reply":"2026-02-12T06:32:25.290923Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# Logistic Reression (Weak Learner 1)\ndef train_logistic(X, y):\n    oof_preds = np.zeros(len(X))\n    scores = []\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n        model = LogisticRegression(max_iter=200)\n        model.fit(X_train, y_train)\n\n        preds = model.predict_proba(X_val)[:,1]\n        oof_preds[val_idx] = preds\n\n        score = roc_auc_score(y_val, preds)\n        scores.append(score)\n\n        print(f\"Fold {fold+1} AUC: {score:.4f}\")\n\n    print(\"Mean AUC:\", np.mean(scores))\n    return np.mean(scores)\n\nbaseline_score = train_logistic(X, y)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T06:42:12.990293Z","iopub.execute_input":"2026-02-12T06:42:12.990595Z","iopub.status.idle":"2026-02-12T06:43:30.139944Z","shell.execute_reply.started":"2026-02-12T06:42:12.990573Z","shell.execute_reply":"2026-02-12T06:43:30.138731Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Fold 1 AUC: 0.9228\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Fold 2 AUC: 0.8731\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Fold 3 AUC: 0.8192\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Fold 4 AUC: 0.9436\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"Fold 5 AUC: 0.9215\nMean AUC: 0.8960421173455053\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"# LightBGM (Weak Learner 2)\ndef train_lgb(X, y):\n    oof_preds = np.zeros(len(X))\n    scores = []\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n        model = lgb.LGBMClassifier(\n            n_estimators=500,\n            learning_rate=0.01,\n            max_depth=5,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            random_state=RANDOM_STATE\n        )\n\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=\"auc\",\n        )\n\n        preds = model.predict_proba(X_val)[:,1]\n        oof_preds[val_idx] = preds\n\n        score = roc_auc_score(y_val, preds)\n        scores.append(score)\n\n        print(f\"Fold {fold+1} AUC: {score:.4f}\")\n\n    print(\"Mean AUC:\", np.mean(scores))\n    return np.mean(scores)\n\nlgb_score = train_lgb(X, y)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T06:54:10.116412Z","iopub.execute_input":"2026-02-12T06:54:10.116685Z","iopub.status.idle":"2026-02-12T06:56:13.787907Z","shell.execute_reply.started":"2026-02-12T06:54:10.116667Z","shell.execute_reply":"2026-02-12T06:56:13.786941Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 225963, number of negative: 278037\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050319 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 677\n[LightGBM] [Info] Number of data points in the train set: 504000, number of used features: 14\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448339 -> initscore=-0.207383\n[LightGBM] [Info] Start training from score -0.207383\nFold 1 AUC: 0.9541\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 225963, number of negative: 278037\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068522 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 672\n[LightGBM] [Info] Number of data points in the train set: 504000, number of used features: 14\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448339 -> initscore=-0.207383\n[LightGBM] [Info] Start training from score -0.207383\nFold 2 AUC: 0.9531\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 225963, number of negative: 278037\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039525 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 669\n[LightGBM] [Info] Number of data points in the train set: 504000, number of used features: 14\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448339 -> initscore=-0.207383\n[LightGBM] [Info] Start training from score -0.207383\nFold 3 AUC: 0.9537\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 225963, number of negative: 278037\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023891 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 671\n[LightGBM] [Info] Number of data points in the train set: 504000, number of used features: 14\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448339 -> initscore=-0.207383\n[LightGBM] [Info] Start training from score -0.207383\nFold 4 AUC: 0.9533\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 225964, number of negative: 278036\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030203 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 672\n[LightGBM] [Info] Number of data points in the train set: 504000, number of used features: 14\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448341 -> initscore=-0.207375\n[LightGBM] [Info] Start training from score -0.207375\nFold 5 AUC: 0.9542\nMean AUC: 0.9536807494590406\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"# y[\"Heart Disease\"] = y[\"Heart Disease\"].replace({\"Presence\":1, \"Absence\": 0})\ny = y.map({\"Absence\": 0, \"Presence\": 1})\n\nRANDOM_STATE = 42\n# XGBoost (L 3)\ndef train_xgb(X, y):\n    oof_preds = np.zeros(len(X))\n    scores = []\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n        model = xgb.XGBClassifier(\n            n_estimators=1000,\n            learning_rate=0.01,\n            max_depth=5,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            random_state=RANDOM_STATE,\n            eval_metric=\"auc\"\n        )\n\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            verbose=False\n        )\n\n        preds = model.predict_proba(X_val)[:,1]\n        score = roc_auc_score(y_val, preds)\n        scores.append(score)\n\n        print(f\"Fold {fold+1} AUC: {score:.4f}\")\n\n    print(\"Mean AUC:\", np.mean(scores))\n    return np.mean(scores)\n\nxgb_score = train_xgb(X, y)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T06:57:12.231924Z","iopub.execute_input":"2026-02-12T06:57:12.232245Z","iopub.status.idle":"2026-02-12T06:59:40.548427Z","shell.execute_reply.started":"2026-02-12T06:57:12.232220Z","shell.execute_reply":"2026-02-12T06:59:40.547860Z"}},"outputs":[{"name":"stdout","text":"Fold 1 AUC: 0.9550\nFold 2 AUC: 0.9541\nFold 3 AUC: 0.9548\nFold 4 AUC: 0.9543\nFold 5 AUC: 0.9551\nMean AUC: 0.9546252226320215\n","output_type":"stream"}],"execution_count":63},{"cell_type":"markdown","source":"# Ensembling","metadata":{}},{"cell_type":"code","source":"def train_blend(X, y):\n    oof_preds = np.zeros(len(X))\n    scores = []\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n\n        lgb_model = lgb.LGBMClassifier(n_estimators=500, learning_rate=0.01)\n        xgb_model = xgb.XGBClassifier(n_estimators=500, learning_rate=0.01)\n\n        lgb_model.fit(X_train, y_train)\n        xgb_model.fit(X_train, y_train)\n\n        preds_lgb = lgb_model.predict_proba(X_val)[:,1]\n        preds_xgb = xgb_model.predict_proba(X_val)[:,1]\n\n        preds = 0.5 * preds_lgb + 0.5 * preds_xgb\n\n        score = roc_auc_score(y_val, preds)\n        scores.append(score)\n\n        print(f\"Fold {fold+1} AUC: {score:.4f}\")\n\n    print(\"Mean AUC:\", np.mean(scores))\n    return np.mean(scores)\n\nblend_score = train_blend(X, y)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T06:59:40.811522Z","iopub.execute_input":"2026-02-12T06:59:40.811864Z","iopub.status.idle":"2026-02-12T07:02:15.892108Z","shell.execute_reply.started":"2026-02-12T06:59:40.811840Z","shell.execute_reply":"2026-02-12T07:02:15.891034Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 225963, number of negative: 278037\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038522 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 671\n[LightGBM] [Info] Number of data points in the train set: 504000, number of used features: 14\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448339 -> initscore=-0.207383\n[LightGBM] [Info] Start training from score -0.207383\nFold 1 AUC: 0.9542\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 225963, number of negative: 278037\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038054 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 671\n[LightGBM] [Info] Number of data points in the train set: 504000, number of used features: 14\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448339 -> initscore=-0.207383\n[LightGBM] [Info] Start training from score -0.207383\nFold 2 AUC: 0.9533\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 225963, number of negative: 278037\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038303 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 671\n[LightGBM] [Info] Number of data points in the train set: 504000, number of used features: 14\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448339 -> initscore=-0.207383\n[LightGBM] [Info] Start training from score -0.207383\nFold 3 AUC: 0.9539\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 225963, number of negative: 278037\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037435 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 674\n[LightGBM] [Info] Number of data points in the train set: 504000, number of used features: 14\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448339 -> initscore=-0.207383\n[LightGBM] [Info] Start training from score -0.207383\nFold 4 AUC: 0.9535\n[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 225964, number of negative: 278036\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030588 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 670\n[LightGBM] [Info] Number of data points in the train set: 504000, number of used features: 14\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448341 -> initscore=-0.207375\n[LightGBM] [Info] Start training from score -0.207375\nFold 5 AUC: 0.9543\nMean AUC: 0.9538314699658577\n","output_type":"stream"}],"execution_count":64},{"cell_type":"markdown","source":"# Final Model","metadata":{}},{"cell_type":"code","source":"final_model = lgb.LGBMClassifier(\n    n_estimators=800,\n    learning_rate=0.01,\n    max_depth=5\n)\n\nfinal_model.fit(X, y)\n\ntest_preds = final_model.predict_proba(test)[:,1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T07:02:50.546702Z","iopub.execute_input":"2026-02-12T07:02:50.547073Z","iopub.status.idle":"2026-02-12T07:03:25.561894Z","shell.execute_reply.started":"2026-02-12T07:02:50.547053Z","shell.execute_reply":"2026-02-12T07:03:25.561107Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n[LightGBM] [Info] Number of positive: 282454, number of negative: 347546\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032592 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 674\n[LightGBM] [Info] Number of data points in the train set: 630000, number of used features: 14\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448340 -> initscore=-0.207381\n[LightGBM] [Info] Start training from score -0.207381\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"submission = pd.DataFrame({\n    \"id\": test[\"id\"],\n    \"Heart Disease\": test_preds\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T07:03:29.304921Z","iopub.execute_input":"2026-02-12T07:03:29.305216Z","iopub.status.idle":"2026-02-12T07:03:29.687627Z","shell.execute_reply.started":"2026-02-12T07:03:29.305196Z","shell.execute_reply":"2026-02-12T07:03:29.685953Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}